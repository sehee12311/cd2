# StyleDrop

<p align="left">
  <a href="https://huggingface.co/spaces/zideliu/styledrop"><img alt="Huggingface" src="https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-StyleDrop-orange"></a>
</p>


This is an unofficial PyTorch implementation of [StyleDrop: Text-to-Image Generation in Any Style](https://arxiv.org/abs/2306.00983).



Unlike the parameters in the paper in (Round 1), we set $\lambda_A=2.0$, $\lambda_B=5.0$ and `d_prj=32`, `is_shared=False`, which we found work better, these hyperparameters can be seen in `configs/custom.py`.

we release them to facilitate community research.


![result1](temp-dir/result/1.png)
<br/><br/>
![result2](temp-dir/result/2.png)
<br/><br/>



## Data & Weights Preparation
VQGAN ë‹¤ìš´ë¡œë“œ [link](https://drive.google.com/file/d/13S_unB87n6KKuuMdyMnyExW0G1kplTbP/view) (from [MAGE](https://github.com/LTH14/mage), thanks!)
ë‹¤ìš´ë¡œë“œ ì´í›„ assets í´ë”ì— vqgan ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì„ ë„£ê³  `assets/vqgan_jax_strongaug.ckpt`.ë¡œ íŒŒì¼ëª… ë³€ê²½

pre-trained ì²´í¬í¬ì¸íŠ¸ ë‹¤ìš´ë¡œë“œ [link](https://huggingface.co/nzl-thu/MUSE/tree/main/assets/ckpts) 
ì´í›„, custom datasetìœ¼ë¡œ ì¶”ê°€ í•™ìŠµ ì§„í–‰

`python extract_empty_feature.py`
ëª…ë ¹ì–´ ì‹¤í–‰ìœ¼ë¡œ í•„ìš”í•œ empty_featureíŒŒì¼ ìƒì„±

ìµœì¢… ë””ë ‰í† ë¦¬ êµ¬ì¡°:
```
.
â”œâ”€â”€ assets
â”‚   â”œâ”€â”€ ckpts
â”‚   â”‚   â”œâ”€â”€ cc3m-285000.ckpt
â”‚   â”‚   â”‚   â”œâ”€â”€ lr_scheduler.pth
â”‚   â”‚   â”‚   â”œâ”€â”€ nnet_ema.pth
â”‚   â”‚   â”‚   â”œâ”€â”€ nnet.pth
â”‚   â”‚   â”‚   â”œâ”€â”€ optimizer.pth
â”‚   â”‚   â”‚   â””â”€â”€ step.pth
â”‚   â”‚   â””â”€â”€ imagenet256-450000.ckpt
â”‚   â”‚       â”œâ”€â”€ lr_scheduler.pth
â”‚   â”‚       â”œâ”€â”€ nnet_ema.pth
â”‚   â”‚       â”œâ”€â”€ nnet.pth
â”‚   â”‚       â”œâ”€â”€ optimizer.pth
â”‚   â”‚       â””â”€â”€ step.pth
â”‚   â”œâ”€â”€ fid_stats
â”‚   â”‚   â”œâ”€â”€ fid_stats_cc3m_val.npz
â”‚   â”‚   â””â”€â”€ fid_stats_imagenet256_guided_diffusion.npz
â”‚   â”œâ”€â”€ pipeline.png
|   â”œâ”€â”€ contexts
â”‚   â”‚   â””â”€â”€ empty_context.npy
â””â”€â”€ â””â”€â”€ vqgan_jax_strongaug.ckpt

```


## Dependencies
```
conda install pytorch torchvision torchaudio cudatoolkit=11.3
pip install accelerate==0.12.0 absl-py ml_collections einops wandb ftfy==6.1.1 transformers==4.23.1 loguru webdataset==0.2.5 gradio
```

## Train
data ë””ë ‰í† ë¦¬ì— ì´ë¯¸ì§€ íŒŒì¼ ìœ„ì¹˜
ì´ë¯¸ì§€ íŒŒì¼ì€ cocoë°ì´í„°ì…‹ í™œìš©
ì¹´í…Œê³ ë¦¬ 80ê°œ 
ì´ë¯¸ì§€ ì•½ 65000ìž¥ -> 4000ìž¥ í™œìš©

1. data/one_style.json íŒŒì¼ ìˆ˜ì •. í˜•ì‹ì€ file_name:[object,style]
   style = Noneìœ¼ë¡œ ì§„í–‰

```json
{"train_4000/COCO_train2014_000000353651.jpg": [
Â  Â  Â  Â  "a toilet bowl on the floor in a tiled bathroom",
Â  Â  Â  Â  "A standing bathroom stall with a metal hose.",
Â  Â  Â  Â  "A close up look at a small white floor drain in a rest room.",
Â  Â  Â  Â  "a rom showing a toilet and a shower",
Â  Â  Â  Â  "A grey tiled shower features a ground toilet."]}
```
2. Training script
```shell
#!/bin/bash
unset EVAL_CKPT
unset ADAPTER
export OUTPUT_DIR="output_dir/for/this/experiment"
accelerate launch --num_processes 8 --mixed_precision fp16 train_t2i_custom_v2.py --config=configs/custom.py
```
configs/custom.py
config.data_path = 'data/one_style.json"
n_steps=10000,batch_size=8
gradio_demo.py ì—ì„œ museëª¨ë¸ì˜ gen_temp ê¸°ì¡´ 4.5 -> 3.0ìœ¼ë¡œ ë³€ê²½

## Inference

pretrained style_adapter ê°€ì¤‘ì¹˜ ë‹¤ìš´ë¡œë“œ [ðŸ¤— Hugging Face](https://huggingface.co/zideliu/StyleDrop/tree/main).
```shell
#!/bin/bash
export EVAL_CKPT="assets/ckpts/cc3m-285000.ckpt" 
export ADAPTER="path/to/your/style_adapter"

export OUTPUT_DIR="output/for/this/experiment"

accelerate launch --num_processes 8 --mixed_precision fp16 train_t2i_custom_v2.py --config=configs/custom.py
```


## Gradio Demo
Put the [style_adapter weights](https://huggingface.co/zideliu/StyleDrop/tree/main) in `./style_adapter` folder and run the following command will launch the demo:

```shell
python gradio_demo.py
```

The demo is also hosted on [HuggingFace](https://huggingface.co/spaces/zideliu/styledrop).

## Citation
```bibtex
@article{sohn2023styledrop,
  title={StyleDrop: Text-to-Image Generation in Any Style},
  author={Sohn, Kihyuk and Ruiz, Nataniel and Lee, Kimin and Chin, Daniel Castro and Blok, Irina and Chang, Huiwen and Barber, Jarred and Jiang, Lu and Entis, Glenn and Li, Yuanzhen and others},
  journal={arXiv preprint arXiv:2306.00983},
  year={2023}
}
```


## Acknowlegment

* The implementation is based on [MUSE-PyTorch](https://github.com/baaivision/MUSE-Pytorch)
* Many thanks for the generous help from [Zanlin Ni](https://github.com/nzl-thu)
